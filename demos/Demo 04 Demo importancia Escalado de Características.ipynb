{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Demo 04 Demo importancia Escalado de Características.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"A8bEG3oO4UqE","colab_type":"text"},"source":["\n","# Importancia del Escalado de Características\n","\n","Escalado de características mediante estandarización (o normalización Z-score) puede ser un paso de preprocesamiento importante para muchos  Algoritmos de ML. La estandarización consiste en reescalar las características\n","que tienen las propiedades de una distribución normal estándar con una media de cero y una desviación estándar de uno.\n","\n","Mientras que muchos algoritmos (tales como SVM, K-K-nearest neighbors, y  regresión logística) requieren características que se normalizan, intuitivamente podemos pensar en el análisis de componentes principales (PCA) como un ejemplo de cuando la normalización es importante. En PCA estamos interesados en la componentes que maximizan la varianza. Si un componente (p. ej., altura) varía menos que otro (por ejemplo, peso) debido a su respectivas escalas (metros vs. kilos), la PCA podría determinar que el Dirección de la varianza máxima se corresponde más estrechamente con el eje \"peso\", si esas entidades no se escalan. Como un cambio en la altura de un metro puede considerarse mucho más importante que la cambio en el peso de un kilogramo, esto es claramente incorrecto.\n","\n","Para ilustrar esto, la PCA se realiza comparando el uso de datos con : clase: ' StandardScaler <sklearn.preprocessing.StandardScaler>' aplicado, a los datos sin escala. Los resultados se visualizan y se observa una clara diferencia.\n","Se puede ver el 1er componente principal en el conjunto sin escala. Se puede ver que la característica #13 domina la dirección, siendo un conjunto de dos órdenes de magnitud por encima de las otras características. Esto se contrasta al observar\n","el componente principal de la versión escalada de los datos. En la escala versión, las órdenes de magnitud son aproximadamente las mismas en todas las características.\n","\n","El conjunto de datos utilizado es el DataSet de vino disponible en UCI. Este conjunto de datos tiene características continuas que son heterogéneas en escala debido a las diferencias propiedades que miden (es decir, el contenido de alcohol y el ácido málico).\n","\n","Los datos transformados se utilizan para entrenar un clasificador Bayes ingenuo, y un diferencia clara en las precisiones de predicción se observa en el que el DataSet que se escala antes de PCA supera enormemente la versión sin escala"]},{"cell_type":"code","metadata":{"id":"X3oQKkzm4UqH","colab_type":"code","colab":{}},"source":["from __future__ import print_function\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn import metrics\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_wine\n","from sklearn.pipeline import make_pipeline\n","print(__doc__)\n","\n","# Code source: Tyler Lanigan <tylerlanigan@gmail.com>\n","#              Sebastian Raschka <mail@sebastianraschka.com>\n","\n","# License: BSD 3 clause\n","\n","RANDOM_STATE = 42\n","FIG_SIZE = (10, 7)\n","\n","\n","features, target = load_wine(return_X_y=True)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zAf5y3HKiu8t","colab_type":"code","colab":{}},"source":["# Make a train/test split using 30% test size\n","X_train, X_test, y_train, y_test = train_test_split(features, target,\n","                                                    test_size=0.30,\n","                                                    random_state=RANDOM_STATE)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L_naY3sfi-vO","colab_type":"code","colab":{}},"source":["\n","# Fit to data and predict using pipelined GNB and PCA.\n","unscaled_clf = make_pipeline(PCA(n_components=2), GaussianNB())\n","unscaled_clf.fit(X_train, y_train)\n","pred_test = unscaled_clf.predict(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mN0fMEuji9YJ","colab_type":"code","colab":{}},"source":["\n","# Fit to data and predict using pipelined scaling, GNB and PCA.\n","std_clf = make_pipeline(StandardScaler(), PCA(n_components=2), GaussianNB())\n","std_clf.fit(X_train, y_train)\n","pred_test_std = std_clf.predict(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xqkGqbgni78C","colab_type":"code","colab":{}},"source":["\n","# Show prediction accuracies in scaled and unscaled data.\n","print('\\nPrediction accuracy for the normal test dataset with PCA')\n","print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test)))\n","\n","print('\\nPrediction accuracy for the standardized test dataset with PCA')\n","print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, pred_test_std)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T2d_K9VQi6b1","colab_type":"code","colab":{}},"source":["# Extract PCA from pipeline\n","pca = unscaled_clf.named_steps['pca']\n","pca_std = std_clf.named_steps['pca']\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sZVF6cmki4Z3","colab_type":"code","colab":{}},"source":["\n","# Show first principal components\n","print('\\nPC 1 without scaling:\\n', pca.components_[0])\n","print('\\nPC 1 with scaling:\\n', pca_std.components_[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7JewpEaVi1i8","colab_type":"code","colab":{}},"source":["# Use PCA without and with scale on X_train data for visualization.\n","X_train_transformed = pca.transform(X_train)\n","scaler = std_clf.named_steps['standardscaler']\n","X_train_std_transformed = pca_std.transform(scaler.transform(X_train))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NzF7buAIixmo","colab_type":"code","colab":{}},"source":["# visualize standardized vs. untouched dataset with PCA performed\n","fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=FIG_SIZE)\n","\n","\n","for l, c, m in zip(range(0, 3), ('blue', 'red', 'green'), ('^', 's', 'o')):\n","    ax1.scatter(X_train_transformed[y_train == l, 0],\n","                X_train_transformed[y_train == l, 1],\n","                color=c,\n","                label='class %s' % l,\n","                alpha=0.5,\n","                marker=m\n","                )\n","\n","for l, c, m in zip(range(0, 3), ('blue', 'red', 'green'), ('^', 's', 'o')):\n","    ax2.scatter(X_train_std_transformed[y_train == l, 0],\n","                X_train_std_transformed[y_train == l, 1],\n","                color=c,\n","                label='class %s' % l,\n","                alpha=0.5,\n","                marker=m\n","                )\n","\n","ax1.set_title('Training dataset after PCA')\n","ax2.set_title('Standardized training dataset after PCA')\n","\n","for ax in (ax1, ax2):\n","    ax.set_xlabel('1st principal component')\n","    ax.set_ylabel('2nd principal component')\n","    ax.legend(loc='upper right')\n","    ax.grid()\n","\n","plt.tight_layout()\n","\n","plt.show()"],"execution_count":0,"outputs":[]}]}